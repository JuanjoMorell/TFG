%!TEX root = proyecto.tex

\chapter*{Extended abstract}
\addcontentsline{toc}{chapter}{Extended abstract}

\vspace{-1cm}
The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing global pandemic of coronavirus disease 2019 (COVID-19) that affects all the human behaviour. The World Health Organization (WHO) declared a Public Health Emergency of International Concern regarding COVID-19 on 30 January 2020, and later declared a pandemic on 11 March 2020. This is why the WHO plublished an interim guidance to provide a updated guidance on mask use in health care and community settings, and during home care for COVID-19 cases \cite{covid19}. This document contains updated evidence and guidance on mask management, virus (COVID-19) transmission, mask use by the public in areas with community and cluster transmission, mask use by the public in areas with community and cluster transmission, mask use during vigorous intensity physical activity, etc. In addition, the World Health Organization (WHO) advises the use of masks as part of a comprehensive package of prevention and control measures to limit the spread of COVID-19 \cite{oms}. For this reason, this work will propose the use of Computer Vision and Machine Learning techniques for the implementation of prototypes capable of carrying out the control of mask use in real time. The effort will be focused on the study of the following techniques: Haar-like feature, Facial Landmarks, Mediapipe, Transfer Learning / Tensorflow.

The first technique, Haar-like feature, comes from an investigation directed by Paul Viola and Michael Jones in 2001, focused on real-time facial recognition using these features and a Machine Learning model called Adaboost. Moreover, the paper \cite{paulViola} describes a machine learning approach for visual object detection which is capable of processing images fast and quite accurate. This investigation can be divided into three key concepts. The first is a new image representation called \textit{Integral Image} which allows the features used compute faster during the execution of the detection. The second is a learning algorithm, based on AdaBoost, which selects a small set of features from a larger set features and produces efficient classifiers. The third one is a method for combining those classifiers into a cascade architecture, which allows discard background regions of the input image and spend more computation on promising object-like regions. Therefore, in real-time applications, a detector which use this technique runs at 15 frames per seconds.

The first prototype is created by using an SVM (Super Vector Machine) model and an OpenCV face detector based on Haar-like features. This model is a supervised learning model with associated learning algorithms that analyze data for classification and regresion analysis, and it's created by detections of the OpenCV's face detector itself, with images of faces with a mask and without a mask. Obtaining a prototype that performs a detection that runs at 15 ms in real time with a good accurate.

The second technique, Facial Landmarks, is implemented in the Dlib library for Python. It allows the recognition of points of interest on the faces that have been detected in the input image. The process that takes Dlib for this technique can be divided into two main steps: detecting faces within the image and obtaining those points of interest. This implementation is based on the research of Kazemi and Sullivan in 2014, with the paper \textit{One Millisecond Face Alignment with an Ensemble of Regression Trees} \cite{faceLandmark}. This technique is capable of recognizing the following points of interest in a face: mouth, eyebrows, eyes, nose and chin, thanks to the use of an ensemble of regression trees, that can be used to estimate this points (\textit{facial landmarks}) directly form a sparse subset of pixel intensities.

The first step is to find a face within the input image, using the HOG (Histogram of Oriented Gradients) method. Which follows an idea like the Haar-like features method since it is based on the detection of features. The theoretical idea behind HOG is to find the appearance and shape of an object by distributing the intensity of local gradients, thanks to the fact that these obtain a greater magnitude in the vicinity of edges or corners. While deploying, it splits the image into small regions, cells, and a one-dimensional gradient histogram is calculated for each of the pixels in each cell. And the second step is to estimate the points of interest accurately and efficiently on the face. It's based on gradient boosting for learning an ensemble of regression trees, in charge of predicting the points of interest. 

Using OpenCV and the Dlib ToolKit, both ideas are implemented using pre-trained models. For facial detection and prediction of points of interest, it will be implemented with models proposed by Dlib, being the detection based on HOG and the prediction called \textit{shape\_predictor\_68\_face\_landmarks.dat}. The prototype created performs detections at 14.7 ms but with poor accurate for this task. 

The third prototype is built with the use of Mediapipe, an API created by Google that offers customizable machine learning solutions for live and streaming media.
Google recommends Mediapipe to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system perfomance and resource consumption on target platforms \cite{mediapipe}. Among the solutions it presents, there is a solution called Face Mesh that offers a solution that estimates 468 points of interest of a face, forming a 3D mesh in real time.

\vspace{-0.3cm}
MediaPipe Face Mesh employs machine learning (ML) to infer the 3D surface geometry, requiring only a simple webcam like camera input. Using lightweight model architectures together with GPU acceleration throughout the pipeline, the solution delivers real-time performance critical for live experiences.

\vspace{-0.3cm}
The pipeline used in this API consists of two Deep Learning models working at the same time. Its functionality is to perform a detection from an image of the points of interest on a face and build a 3D face landmark model that approximates its surface by regression on said points. This task is facilitated if the face, where the points of interest must be detected, is clipped, thus making the model focus only on finding the points, increasing the precision of the prediction. Likewise, the cutouts of the faces can be generated from the previous predictions made by the same model, and the prediction is only called again when the presence of the face cannot be detected \cite{faceMesh}. All this is implemented thanks to the \textit{MediaPipe framework}. A pipeline is defined as a directed graph of components where each component is a Calculator. These calculators are connected by data Streams. Each one represents a time-series of data Packets. So, the calculators and streams define a data-flow graph \cite{mediapipe}.

\vspace{-0.3cm}
The prototype will be made using FaceMesh from the MediaPipe API and a detector based on Haar-like features, such as the one mentioned in the Viola and Jones paper. Therefore, the idea behind this prototype is to obtain the area of the mouth thanks to the use of the solution provided by Mediapipe. And detect if there is a mouth in that area using a Haar-like features model implemented in OpenCV with a pre-trained model. The prototype makes detections at 12.3 ms with good precision and accuracy at close distances. 

\vspace{-0.3cm}
The last prototype uses the Transfer Learning technique, and for that the use of Tensorflow is necessary. It is an Open Source platform dedicated to machine learning. Allows you to easily compile and deploy ML technology applications. It is based on tensors, multidimensional arrays with a uniform type, like np.arrays in Numpy. In the field of computer vision, it has a large number of pre-trained models capable of performing object detection and image classification quickly and accurately. This resource is called TensorFlow Model Garden, and it is a repository with different model implementations and solutions modeled for Tensorflow. The models are pre-trained using a dataset called COCO 2017. This being a large data set on a large scale for object detection, segmentation, and capture. Among these models, MobileNet stands out, capable of obtaining great performance on mobile devices and computers with little computational power. This has created a rise in popularity for detector implementations on embedded devices, such as the Raspberry Pi, or on ESP32 chips using remote processing.

\vspace{-0.3cm}
The goal in this prototype is to use Transfer Learning on the MobileNet model to retrain it for our task. Transfer Learning is a technique in which a pre-trained model is reused for a new problem. Lately its use is becoming popular, since it allows the training of a deep neural network with a small amount of data. In this case, the aim is to use a general object detection model and, applying transfer learning, re-train it for a more specific task but without wasting its previous knowledge. 

\vspace{-0.3cm}
The prototype will be built with the use of an SSD-MobileNet model, a combination of an SSD (Single Shot MultiBox Detector) model with another called MobileNet. This type of model is characterized by the use of a method to detect objects in images using a deep neural network, which generates prediction values about the presence of each object / category in each of the detections and returns the object with which the most match. The dataset used in Transfer Learning is a combination of datasets composed of images of people with a mask, without it and with it, but incorrectly placed. It consists of a total of 895 images, divided into two groups: train and test. The first contains the images that will be used to train the model and has 745 images. Whereas, the test set will be used to validate the trained model has 150 images. Once the images are selected, the labeling process is carried out, using a program created with Python called labelImg. 

\vspace{-0.3cm}
To perform Transfer Learning using Tensorflow, the following configuration files need to be created: \textit{LabelMap}, two \textit{TFRecord} files, and \textit{pipeline.config}. This last file contains the main information of the model, both the number of classes that it can detect, the size that the images readjusts before treating them and checkpoints for training. For its creation the use of Protobuf will be necessary. Google tool focused on transforming the xml files generated by labeling the images in protocol buffers, to serialize the information in a structured and faster way. To carry out the training, use is made of the code provided after the installation of the Tensorflow Object Detection API, called \textit{model\_main\_tf2}. After training, files called checkpoints are obtained with which the model can be loaded for use through Tensorflow. Once the prototype is created, detections are made in a time of 83.3 ms. Making detections slower than the rest of the prototypes created but being a more scalable project.

\vspace{-0.3cm}
Tree metrics are used to compare the prototypes. The first comparison is based on the study of the working range of the prototypes. Being able to measure its distance by the size of the detection that the algorithm implemented in the prototype can performing. The second comparison is based on the measure of time the algorithm takes to process an input image. For this, the same test will be carried out on all prototypes, the average time (ms) is calculated after an execution of one minute. Finally, the third comparison will obtain the percentage of success for the dataset built in this project.

\vspace{-0.3cm}
As a result, there is no single prototype capable of standing out across the board, making it impossible to select a general solution to the problem. All the techniques used in this work can solve it and depending on where the prototype must be implemented, another study would have to be carried out to solve these specific objectives. For example, if you want to implement a detector of this type in a mobile device or in an embedded system, you will look for a prototype that is fast and light, such as the third prototype (Mediapipe). Whereas, if you have a device with more computing power, you could consider using a slower but more precise prototype like the first one (Haar-like features) or even training a custom model like the fourth prototype (Transfer Learning). Even so, a preliminary study should be carried out on the device where the prototype will be implemented to make a final decision.


